#!/bin/bash

# Script para verificar y configurar GPU NVIDIA automÃ¡ticamente
# Autor: Script generado para AgentOllamaAi

set -e

# Colores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${BLUE}ğŸ”„ ConfiguraciÃ³n automÃ¡tica de IA local (con detecciÃ³n GPU mejorada)...${NC}"

# FunciÃ³n para verificar puerto
check_port() {
    local port=$1
    if ss -tulpn | grep ":$port " > /dev/null 2>&1; then
        return 0  # Puerto ocupado
    else
        return 1  # Puerto libre
    fi
}

# 1. Limpieza automÃ¡tica sin confirmaciÃ³n
echo -e "${YELLOW}ğŸ§¹ Paso 1: Limpieza automÃ¡tica...${NC}"

# Detener servicios Docker Compose
echo "Deteniendo servicios Docker Compose..."
docker-compose down --remove-orphans 2>/dev/null || true
docker compose down --remove-orphans 2>/dev/null || true

# Detener y eliminar contenedores relacionados
echo "Eliminando contenedores relacionados..."
docker stop $(docker ps -q --filter "name=ollama") 2>/dev/null || true
docker stop $(docker ps -q --filter "name=open-webui") 2>/dev/null || true
docker rm $(docker ps -aq --filter "name=ollama") 2>/dev/null || true
docker rm $(docker ps -aq --filter "name=open-webui") 2>/dev/null || true

# Liberar puertos 11434 y 3000 de forma agresiva
echo "Liberando puertos 11434 y 3000..."
if check_port 11434; then
    echo "Puerto 11434 ocupado, liberando..."
    pkill -f "docker-proxy.*11434" 2>/dev/null || true
fi

if check_port 3000; then
    echo "Puerto 3000 ocupado, liberando..."
    pkill -f "docker-proxy.*3000" 2>/dev/null || true
fi

sleep 3

# Limpiar recursos Docker automÃ¡ticamente
echo "Limpiando recursos Docker..."
docker network prune -f 2>/dev/null || true
docker volume prune -f 2>/dev/null || true

echo -e "${GREEN}âœ… Limpieza completada${NC}"

# 2. Verificar que los puertos estÃ©n libres
echo -e "${YELLOW}ğŸ” Paso 2: Verificando puertos 11434 y 3000...${NC}"
if check_port 11434; then
    echo -e "${RED}âŒ El puerto 11434 aÃºn estÃ¡ ocupado. Intentando una vez mÃ¡s...${NC}"
    pkill -f "docker.*11434" 2>/dev/null || true
fi

if check_port 3000; then
    echo -e "${RED}âŒ El puerto 3000 aÃºn estÃ¡ ocupado. Intentando una vez mÃ¡s...${NC}"
    pkill -f "docker.*3000" 2>/dev/null || true
    # TambiÃ©n intentar matar procesos node/next.js que usen el puerto 3000
    lsof -ti :3000 | xargs kill -9 2>/dev/null || true
fi

sleep 3

# VerificaciÃ³n final de puertos
if check_port 11434 || check_port 3000; then
    echo -e "${RED}âŒ No se pudieron liberar todos los puertos necesarios.${NC}"
    echo "Puertos ocupados:"
    sudo netstat -tlnp | grep -E ":(11434|3000) " || true
    exit 1
fi

echo -e "${GREEN}âœ… Puertos 11434 y 3000 libres${NC}"

# 3. Detectar GPU de forma mÃ¡s robusta
echo -e "${YELLOW}ğŸ” Paso 3: Detectando hardware y capacidades GPU...${NC}"

# Verificar si NVIDIA estÃ¡ disponible
NVIDIA_AVAILABLE=false
GPU_COMPATIBLE=false

# Verificar comando nvidia-smi
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… nvidia-smi encontrado"
    if nvidia-smi > /dev/null 2>&1; then
        echo "âœ… nvidia-smi funciona correctamente"
        NVIDIA_AVAILABLE=true
    else
        echo "âš ï¸ nvidia-smi no funciona correctamente"
    fi
else
    echo "âš ï¸ nvidia-smi no encontrado"
fi

# Verificar Docker con GPU support
if $NVIDIA_AVAILABLE; then
    echo "ğŸ” Verificando soporte GPU en Docker..."
    if docker run --rm --runtime=nvidia --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi &> /dev/null; then
        echo "âœ… Docker puede usar GPU NVIDIA"
        GPU_COMPATIBLE=true
    else
        echo "âš ï¸ Docker no puede usar GPU NVIDIA correctamente"
    fi
fi

# Decidir perfil basado en compatibilidad completa
if $GPU_COMPATIBLE; then
    echo -e "${GREEN}âœ… GPU NVIDIA completamente compatible${NC}"
    PROFILE="gpu"
else
    echo -e "${YELLOW}âš ï¸ Usando CPU (GPU no disponible o no compatible)${NC}"
    PROFILE="cpu"
fi

# 4. Iniciar servicios
echo -e "${YELLOW}ğŸ³ Paso 4: Iniciando servicios con perfil $PROFILE...${NC}"
docker compose --profile $PROFILE up -d

# Esperar a que los servicios estÃ©n listos
echo "â³ Esperando a que Ollama estÃ© listo..."
sleep 15

# Verificar que Ollama estÃ© respondiendo
echo "ğŸ” Verificando conectividad con Ollama..."
for i in {1..10}; do
    if curl -s http://localhost:11434/api/version > /dev/null; then
        echo -e "${GREEN}âœ… Ollama estÃ¡ funcionando${NC}"
        break
    else
        echo "Intento $i/10: Esperando a Ollama..."
        sleep 5
    fi
    
    if [ $i -eq 10 ]; then
        echo -e "${RED}âŒ Ollama no responde despuÃ©s de 50 segundos${NC}"
        echo "Verificando logs del contenedor..."
        docker logs ollama-dev-ai 2>/dev/null || true
        exit 1
    fi
done

# 5. Descargar solo modelos esenciales automÃ¡ticamente
echo -e "${YELLOW}ğŸ“¥ Paso 5: Descargando modelos esenciales...${NC}"

CONTAINER_NAME="ollama-dev-ai"

# Solo modelos bÃ¡sicos y rÃ¡pidos para empezar
echo "ğŸ“¦ Descargando modelos bÃ¡sicos (esto puede tomar varios minutos)..."
echo "   - Descargando llama3.2:1b (modelo mÃ¡s rÃ¡pido)..."
docker exec $CONTAINER_NAME ollama pull llama3.2:1b

echo "   - Descargando qwen2.5-coder:7b (especialista en cÃ³digo)..."
docker exec $CONTAINER_NAME ollama pull qwen2.5-coder:7b

echo "   - Descargando deepseek-coder:6.7b (excelente para programaciÃ³n)..."
docker exec $CONTAINER_NAME ollama pull deepseek-coder:6.7b

echo "   - Descargando codellama:7b (modelo general de cÃ³digo)..."
docker exec $CONTAINER_NAME ollama pull codellama:7b

echo "   - Descargando llama3.1:8b (modelo versÃ¡til)..."
docker exec $CONTAINER_NAME ollama pull llama3.1:8b

# 6. Actualizar configuraciÃ³n de Continue
echo -e "${YELLOW}ğŸ”§ Paso 6: Actualizando configuraciÃ³n de Continue...${NC}"
if [ -f "update-continue-config.sh" ]; then
    chmod +x update-continue-config.sh
    ./update-continue-config.sh 2>/dev/null || true
    echo -e "${GREEN}âœ… ConfiguraciÃ³n de Continue actualizada${NC}"
fi

# 7. Copiar configuraciÃ³n final
echo -e "${YELLOW}ğŸ”§ Paso 7: Aplicando configuraciÃ³n final...${NC}"
if [ -f "continue-config.json" ]; then
    mkdir -p "$HOME/.continue"
    cp "continue-config.json" "$HOME/.continue/config.json"
    echo -e "${GREEN}âœ… ConfiguraciÃ³n copiada${NC}"
fi

# 8. VerificaciÃ³n final
echo -e "${BLUE}ğŸ¯ VerificaciÃ³n final:${NC}"
echo "ğŸ“‹ Contenedores activos:"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

echo ""
echo "ğŸ§  Modelos disponibles:"
docker exec $CONTAINER_NAME ollama list

echo ""
echo -e "${GREEN}ğŸ‰ Â¡ConfiguraciÃ³n completada exitosamente!${NC}"
echo -e "${BLUE}ğŸ”— URLs disponibles:${NC}"
echo "   â€¢ Ollama API: http://localhost:11434"
echo "   â€¢ Web UI: http://localhost:3001"
echo ""
echo -e "${YELLOW}ğŸ“ PrÃ³ximos pasos:${NC}"
echo "1. Reinicia VS Code completamente"
echo "2. Abre Continue desde la barra lateral"
echo "3. Los modelos deberÃ­an aparecer automÃ¡ticamente"
echo ""
echo -e "${GREEN}ğŸ’¡ Si tienes problemas, ejecuta: ./check-continue-status.sh${NC}"

# Mostrar informaciÃ³n del perfil usado
echo ""
echo -e "${BLUE}â„¹ï¸  InformaciÃ³n del sistema:${NC}"
echo "   â€¢ Perfil usado: $PROFILE"
if $NVIDIA_AVAILABLE; then
    echo "   â€¢ GPU NVIDIA: Detectada"
    if $GPU_COMPATIBLE; then
        echo "   â€¢ Compatibilidad Docker+GPU: âœ… Completa"
    else
        echo "   â€¢ Compatibilidad Docker+GPU: âŒ Problemas detectados"
    fi
else
    echo "   â€¢ GPU NVIDIA: No detectada"
fi
