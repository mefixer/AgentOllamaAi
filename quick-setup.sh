#!/bin/bash

# Script autom√°tico para limpiar y configurar la IA local sin interacci√≥n
# Autor: Script generado para AgentOllamaAi

set -e

# Colores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${BLUE}üîÑ Configuraci√≥n autom√°tica de IA local...${NC}"

# Funci√≥n para verificar puerto
check_port() {
    local port=$1
    if ss -tulpn | grep ":$port " > /dev/null 2>&1; then
        return 0  # Puerto ocupado
    else
        return 1  # Puerto libre
    fi
}

# 1. Limpieza autom√°tica sin confirmaci√≥n
echo -e "${YELLOW}üßπ Paso 1: Limpieza autom√°tica...${NC}"

# Detener servicios Docker Compose
echo "Deteniendo servicios Docker Compose..."
docker-compose down --remove-orphans 2>/dev/null || true
docker compose down --remove-orphans 2>/dev/null || true

# Detener y eliminar contenedores relacionados
echo "Eliminando contenedores relacionados..."
docker stop $(docker ps -q --filter "name=ollama") 2>/dev/null || true
docker stop $(docker ps -q --filter "name=open-webui") 2>/dev/null || true
docker rm $(docker ps -aq --filter "name=ollama") 2>/dev/null || true
docker rm $(docker ps -aq --filter "name=open-webui") 2>/dev/null || true

# Liberar puerto 11434 de forma agresiva
echo "Liberando puerto 11434..."
if check_port 11434; then
    echo "Puerto 11434 ocupado, liberando..."
    pkill -f "docker-proxy.*11434" 2>/dev/null || true
    sleep 3
fi

# Limpiar recursos Docker autom√°ticamente
echo "Limpiando recursos Docker..."
docker network prune -f 2>/dev/null || true
docker volume prune -f 2>/dev/null || true

echo -e "${GREEN}‚úÖ Limpieza completada${NC}"

# 2. Verificar que el puerto est√© libre
echo -e "${YELLOW}üîç Paso 2: Verificando puerto 11434...${NC}"
if check_port 11434; then
    echo -e "${RED}‚ùå El puerto 11434 a√∫n est√° ocupado. Intentando una vez m√°s...${NC}"
    pkill -f "docker.*11434" 2>/dev/null || true
    sleep 3
    if check_port 11434; then
        echo -e "${RED}‚ùå No se pudo liberar el puerto. Verifica manualmente.${NC}"
        exit 1
    fi
fi
echo -e "${GREEN}‚úÖ Puerto 11434 libre${NC}"

# 3. Detectar GPU y configurar perfil
echo -e "${YELLOW}üîç Paso 3: Detectando hardware...${NC}"
if command -v nvidia-smi &> /dev/null && nvidia-smi > /dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ GPU NVIDIA detectada${NC}"
    PROFILE="gpu"
else
    echo -e "${YELLOW}‚ö†Ô∏è  No se detect√≥ GPU NVIDIA, usando CPU${NC}"
    PROFILE="cpu"
fi

# 4. Iniciar servicios
echo -e "${YELLOW}üê≥ Paso 4: Iniciando servicios con perfil $PROFILE...${NC}"
docker compose --profile $PROFILE up -d

# Esperar a que los servicios est√©n listos
echo "‚è≥ Esperando a que Ollama est√© listo..."
sleep 15

# Verificar que Ollama est√© respondiendo
echo "üîç Verificando conectividad con Ollama..."
for i in {1..10}; do
    if curl -s http://localhost:11434/api/version > /dev/null; then
        echo -e "${GREEN}‚úÖ Ollama est√° funcionando${NC}"
        break
    else
        echo "Intento $i/10: Esperando a Ollama..."
        sleep 5
    fi
    
    if [ $i -eq 10 ]; then
        echo -e "${RED}‚ùå Ollama no responde despu√©s de 50 segundos${NC}"
        exit 1
    fi
done

# 5. Descargar solo modelos esenciales autom√°ticamente
echo -e "${YELLOW}üì• Paso 5: Descargando modelos esenciales...${NC}"

CONTAINER_NAME="ollama-dev-ai"

# Solo modelos b√°sicos y r√°pidos para empezar
echo "üì¶ Descargando modelos b√°sicos..."
docker exec $CONTAINER_NAME ollama pull llama3.2:1b
docker exec $CONTAINER_NAME ollama pull qwen2.5-coder:7b
docker exec $CONTAINER_NAME ollama pull deepseek-coder:6.7b
docker exec $CONTAINER_NAME ollama pull codellama:7b
docker exec $CONTAINER_NAME ollama pull llama3.1:8b

# 6. Actualizar configuraci√≥n de Continue
echo -e "${YELLOW}üîß Paso 6: Actualizando configuraci√≥n de Continue...${NC}"
if [ -f "update-continue-config.sh" ]; then
    chmod +x update-continue-config.sh
    ./update-continue-config.sh
    echo -e "${GREEN}‚úÖ Configuraci√≥n de Continue actualizada${NC}"
fi

# 7. Copiar configuraci√≥n final
echo -e "${YELLOW}üîß Paso 7: Aplicando configuraci√≥n final...${NC}"
if [ -f "continue-config.json" ]; then
    mkdir -p "$HOME/.continue"
    cp "continue-config.json" "$HOME/.continue/config.json"
    echo -e "${GREEN}‚úÖ Configuraci√≥n copiada${NC}"
fi

# 8. Verificaci√≥n final
echo -e "${BLUE}üéØ Verificaci√≥n final:${NC}"
echo "üìã Contenedores activos:"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

echo ""
echo "üß† Modelos disponibles:"
docker exec $CONTAINER_NAME ollama list

echo ""
echo -e "${GREEN}üéâ ¬°Configuraci√≥n completada exitosamente!${NC}"
echo -e "${BLUE}üîó URLs disponibles:${NC}"
echo "   ‚Ä¢ Ollama API: http://localhost:11434"
echo "   ‚Ä¢ Web UI: http://localhost:3000"
echo ""
echo -e "${YELLOW}üìù Pr√≥ximos pasos:${NC}"
echo "1. Reinicia VS Code completamente"
echo "2. Abre Continue desde la barra lateral"
echo "3. Los modelos deber√≠an aparecer autom√°ticamente"
echo ""
echo -e "${GREEN}üí° Si tienes problemas, ejecuta: ./check-continue-status.sh${NC}"
