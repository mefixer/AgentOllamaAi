#!/bin/bash

# Script para configurar una IA local para desarrollo con los mejores modelos de c√≥digo abierto

echo "ü§ñ Configurando IA local para desarrollo..."

# Funci√≥n para limpiar recursos previos
cleanup_previous_setup() {
    echo "üßπ Limpiando configuraci√≥n anterior..."
    
    # Detener y eliminar contenedores relacionados con Ollama
    echo "üõë Deteniendo contenedores existentes..."
    docker stop $(docker ps -q --filter "name=ollama") 2>/dev/null || true
    docker stop $(docker ps -q --filter "name=open-webui") 2>/dev/null || true
    
    # Eliminar contenedores
    echo "üóëÔ∏è  Eliminando contenedores antiguos..."
    docker rm $(docker ps -aq --filter "name=ollama") 2>/dev/null || true
    docker rm $(docker ps -aq --filter "name=open-webui") 2>/dev/null || true
    
    # Eliminar redes personalizadas (pero mantener las predeterminadas)
    echo "üåê Limpiando redes no utilizadas..."
    docker network prune -f 2>/dev/null || true
    
    # Limpiar vol√∫menes hu√©rfanos
    echo "üíæ Limpiando vol√∫menes no utilizados..."
    docker volume prune -f 2>/dev/null || true
    
    # Verificar que el puerto 11434 est√© liberado
    echo "üîç Verificando liberaci√≥n del puerto 11434..."
    if netstat -tlnp 2>/dev/null | grep :11434 > /dev/null; then
        echo "‚ö†Ô∏è  El puerto 11434 a√∫n est√° ocupado. Esperando 5 segundos..."
        sleep 5
        # Intentar matar procesos que usen el puerto (con precauci√≥n)
        sudo pkill -f "docker-proxy.*11434" 2>/dev/null || true
        sleep 2
    fi
    
    echo "‚úÖ Limpieza completada"
}

# Ejecutar limpieza antes de configurar
cleanup_previous_setup

# Detectar si el sistema tiene GPU NVIDIA
if command -v nvidia-smi &> /dev/null; then
    echo "‚úÖ GPU NVIDIA detectada"
    PROFILE="gpu"
else
    echo "‚ö†Ô∏è  No se detect√≥ GPU NVIDIA, usando CPU"
    PROFILE="cpu"
fi

# Levantar los servicios
echo "üê≥ Iniciando servicios Docker..."
docker compose --profile $PROFILE up -d

# Esperar a que Ollama est√© listo
echo "‚è≥ Esperando a que Ollama est√© listo..."
sleep 10

# Descargar los mejores modelos de c√≥digo abierto para desarrollo
echo "üì• Descargando modelos de IA para desarrollo..."

# CodeLlama 34B - Excelente para c√≥digo
echo "Descargando CodeLlama 34B (especializado en c√≥digo)..."
docker exec ollama-dev-ai-${PROFILE} ollama pull codellama:34b

# Llama 3.1 8B - Muy bueno y r√°pido
echo "Descargando Llama 3.1 8B (r√°pido y eficiente)..."
docker exec ollama-dev-ai-${PROFILE} ollama pull llama3.1:8b

# DeepSeek Coder 33B - Especializado en programaci√≥n
echo "Descargando DeepSeek Coder 33B (especialista en c√≥digo)..."
docker exec ollama-dev-ai-${PROFILE} ollama pull deepseek-coder:33b

# Qwen 2.5 Coder 32B - Uno de los mejores para c√≥digo
echo "Descargando Qwen 2.5 Coder 32B (excelente para c√≥digo)..."
docker exec ollama-dev-ai-${PROFILE} ollama pull qwen2.5-coder:32b

# Granite Code 34B - Modelo de IBM para c√≥digo
echo "Descargando Granite Code 34B (modelo de IBM)..."
docker exec ollama-dev-ai-${PROFILE} ollama pull granite-code:34b

# Establecer modelo por defecto
echo "üéØ Configurando modelo por defecto..."
docker exec ollama-dev-ai-${PROFILE} ollama pull qwen2.5-coder:7b

echo "‚úÖ ¬°Configuraci√≥n completada!"
echo ""
echo "üîó URLs disponibles:"
echo "   ‚Ä¢ Ollama API: http://localhost:11434"
echo "   ‚Ä¢ Web UI: http://localhost:3001"
echo ""
echo "üìù Modelos instalados:"
echo "   ‚Ä¢ qwen2.5-coder:32b (recomendado para c√≥digo complejo)"
echo "   ‚Ä¢ qwen2.5-coder:7b (r√°pido para c√≥digo simple)"
echo "   ‚Ä¢ codellama:34b (especialista en c√≥digo)"
echo "   ‚Ä¢ llama3.1:8b (uso general r√°pido)"
echo "   ‚Ä¢ deepseek-coder:33b (experto en programaci√≥n)"
echo "   ‚Ä¢ granite-code:34b (modelo IBM para c√≥digo)"
echo ""
echo "üõ†Ô∏è  Para usar en VS Code, instala la extensi√≥n 'Continue' y config√∫rala con:"
echo "   URL: http://localhost:11434"
echo "   Modelo recomendado: qwen2.5-coder:7b"
