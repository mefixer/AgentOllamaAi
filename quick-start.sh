#!/bin/bash

# Script de inicio rÃ¡pido para la IA local

echo "ğŸš€ Inicio RÃ¡pido - IA Local para Desarrollo"
echo ""

# Verificar Docker
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker no estÃ¡ instalado. InstÃ¡lalo primero:"
    echo "   sudo apt-get update && sudo apt-get install docker.io"
    exit 1
fi

# Verificar si Docker estÃ¡ corriendo
if ! docker info &> /dev/null; then
    echo "âŒ Docker no estÃ¡ corriendo. Inicia el servicio:"
    echo "   sudo systemctl start docker"
    exit 1
fi

echo "âœ… Docker verificado"

# Verificar si ya hay servicios corriendo
if docker ps | grep -q "ollama"; then
    echo "âš ï¸  Ya hay servicios de Ollama corriendo"
    echo ""
    ./manage-models.sh status
    exit 0
fi

echo ""
echo "ğŸ¤– Opciones de instalaciÃ³n:"
echo "1. RÃ¡pida (solo modelos ligeros - ~20GB)"
echo "2. Recomendada (modelos balanceados - ~35GB)" 
echo "3. Completa (todos los modelos - ~150GB)"
echo "4. Solo iniciar servicios (sin descargar modelos)"
echo ""
read -p "Selecciona una opciÃ³n (1-4): " option

case $option in
    1)
        echo "ğŸ“¦ InstalaciÃ³n rÃ¡pida seleccionada"
        INSTALL_TYPE="lightweight"
        ;;
    2)
        echo "ğŸ“¦ InstalaciÃ³n recomendada seleccionada"
        INSTALL_TYPE="recommended"
        ;;
    3)
        echo "ğŸ“¦ InstalaciÃ³n completa seleccionada"
        INSTALL_TYPE="full"
        ;;
    4)
        echo "ğŸ“¦ Solo iniciando servicios"
        INSTALL_TYPE="none"
        ;;
    *)
        echo "âŒ OpciÃ³n invÃ¡lida"
        exit 1
        ;;
esac

# Detectar GPU
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… GPU NVIDIA detectada"
    PROFILE="gpu"
else
    echo "â„¹ï¸  Usando CPU (sin GPU detectada)"
    PROFILE="cpu"
fi

# Iniciar servicios
echo "ğŸ³ Iniciando servicios..."
docker compose --profile $PROFILE up -d

# Esperar a que Ollama estÃ© listo
echo "â³ Esperando a que Ollama estÃ© listo..."
sleep 15

# Instalar modelos segÃºn la opciÃ³n seleccionada
if [ "$INSTALL_TYPE" != "none" ]; then
    echo "ğŸ“¥ Instalando modelos ($INSTALL_TYPE)..."
    ./manage-models.sh $INSTALL_TYPE
fi

echo ""
echo "âœ… Â¡InstalaciÃ³n completada!"
echo ""
echo "ğŸ”— Accesos disponibles:"
echo "   â€¢ API de Ollama: http://localhost:11434"
echo "   â€¢ Interfaz Web: http://localhost:3000"
echo ""
echo "ğŸ“ PrÃ³ximos pasos:"
echo "1. Instala la extensiÃ³n 'Continue' en VS Code"
echo "2. Copia el archivo continue-config.json a ~/.continue/config.json"
echo "3. Reinicia VS Code"
echo "4. Usa Ctrl+I para autocompletado inline"
echo ""
echo "ğŸ’¡ Comandos Ãºtiles:"
echo "   ./manage-models.sh status    # Ver estado"
echo "   ./manage-models.sh list      # Listar modelos"
echo "   ./manage-models.sh chat <modelo>  # Chat con modelo"
echo ""
echo "Â¡Disfruta tu IA local! ğŸ‰"
