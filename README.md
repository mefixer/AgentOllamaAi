# IA Local para Desarrollo con Docker ü§ñ

Esta confi### Opci√≥n 3: Instalaci√≥n Limpia (Completa) ‚ú®

Si necesitas todos los modelos o quieres control manual:

```bash
./setup-ai-clean.sh
```

**Caracter√≠sticas:**
- üßπ Limpieza manual con confirmaciones
- üì¶ Opci√≥n de descargar modelos grandes
- üîß Control total del proceso
- ‚è±Ô∏è Tiempo: 30-60 minutos

### Opci√≥n 4: Instalaci√≥n Est√°ndar

```bash
./setup-ai.sh
```

### Opci√≥n 5: Solo Limpieza

Si solo necesitas limpiar la configuraci√≥n anterior:

```bash
./cleanup-ai.sh
```rmite ejecutar una IA local potente para desarrollo usando Docker y los mejores modelos de c√≥digo abierto disponibles.

## üöÄ Caracter√≠sticas

- **Ollama** como servidor de IA local
- **Modelos de c√≥digo abierto** m√°s avanzados para programaci√≥n
- **Interfaz web** para gesti√≥n de modelos
- **Configuraci√≥n autom√°tica** con un solo comando
- **Compatible con GPU y CPU**
- **Integraci√≥n perfecta con VS Code**

## üìã Requisitos Previos

- Docker y Docker Compose instalados
- Al menos 8GB de RAM (16GB recomendado)
- 50GB de espacio libre en disco
- (Opcional) GPU NVIDIA con drivers instalados para mejor rendimiento
- **Para GPU NVIDIA**: NVIDIA Container Toolkit instalado

## üõ†Ô∏è Instalaci√≥n R√°pida

### Opci√≥n 1: Configuraci√≥n Simple (Recomendada) ‚ö°

Para una configuraci√≥n r√°pida sin conflictos de puertos:

```bash
./simple-setup.sh
```

**Caracter√≠sticas:**
- ‚úÖ Puerto 3001 (evita conflictos con el 3000)
- ‚úÖ Limpieza autom√°tica de puertos ocupados
- ‚úÖ Solo modelos esenciales (15-20 minutos)
- ‚úÖ Detecci√≥n inteligente de GPU
- ‚úÖ Configuraci√≥n directa de Continue

### Opci√≥n 2: Configuraci√≥n Autom√°tica (M√°s R√°pida) ‚ö°

Para una configuraci√≥n completamente autom√°tica sin interrupciones:

```bash
./quick-setup.sh
```

**Caracter√≠sticas:**
- ‚úÖ Limpieza autom√°tica sin confirmaciones
- ‚úÖ Descarga solo modelos esenciales (m√°s r√°pido)
- ‚úÖ Configuraci√≥n completa en ~20-30 minutos
- ‚úÖ No requiere interacci√≥n del usuario

### Opci√≥n 2: Instalaci√≥n Limpia (Completa) ‚ú®

Si necesitas todos los modelos o quieres control manual:

```bash
./setup-ai-clean.sh
```

**Caracter√≠sticas:**
- üßπ Limpieza manual con confirmaciones
- ÔøΩ Opci√≥n de descargar modelos grandes
- üîß Control total del proceso
- ‚è±Ô∏è Tiempo: 30-60 minutos

### Opci√≥n 3: Instalaci√≥n Est√°ndar

```bash
./setup-ai.sh
```

### Opci√≥n 4: Solo Limpieza

Si solo necesitas limpiar la configuraci√≥n anterior:

```bash
./cleanup-ai.sh
```

## üß† Modelos Incluidos

### Modelos Especializados en C√≥digo

- **Qwen 2.5 Coder 32B**: El m√°s avanzado para tareas complejas de c√≥digo
- **Qwen 2.5 Coder 7B**: R√°pido y eficiente para c√≥digo simple
- **CodeLlama 34B**: Especialista en m√∫ltiples lenguajes de programaci√≥n
- **DeepSeek Coder 33B**: Excelente para an√°lisis y generaci√≥n de c√≥digo
- **Granite Code 34B**: Modelo de IBM optimizado para desarrollo

### Modelo de Uso General

- **Llama 3.1 8B**: R√°pido para consultas generales y explicaciones

## üîß Configuraci√≥n en VS Code

### Opci√≥n 1: Continue con Agentes (Recomendada) ü§ñ

1. Instala la extensi√≥n **Continue** desde el marketplace
2. Ejecuta el script de configuraci√≥n autom√°tica:
```bash
./setup-continue-agents.sh
```
3. Reinicia VS Code
4. Abre Continue con `Ctrl+Shift+P` > "Continue: Open"
5. **¬°Ahora ver√°s la secci√≥n de Agentes!** Incluye:
   - **Asistente de C√≥digo**: Especialista en desarrollo y debugging
   - **Revisor de C√≥digo**: Analiza errores y vulnerabilidades
   - **Documentador**: Genera documentaci√≥n t√©cnica

### Verificar Configuraci√≥n de Agentes
```bash
./check-continue-status.sh
```

### Opci√≥n 2: Configuraci√≥n Manual

1. Copia el archivo `continue-config.json` a `~/.continue/config.json`
2. Reinicia VS Code
3. Usa `Ctrl+I` para chat inline o `Ctrl+Shift+P` > "Continue: Open"

### Opci√≥n 2: Otras extensiones compatibles

- **Codeium**: Configura con URL `http://localhost:11434`
- **Tabnine**: Usar modo local con la API de Ollama
- **GitHub Copilot Chat**: Configurar con endpoint local

## üåê URLs de Acceso

- **API de Ollama**: http://localhost:11434
- **Interfaz Web**: http://localhost:3001
- **Documentaci√≥n API**: http://localhost:11434/api/tags

## üìù Comandos √ötiles

### Gesti√≥n de Contenedores
```bash
# Iniciar servicios
docker compose --profile cpu up -d  # Para CPU
docker compose --profile gpu up -d  # Para GPU

# Parar servicios
docker compose down

# Ver logs
docker compose logs -f

# Reiniciar servicios
docker compose restart
```

### Gesti√≥n de Modelos
```bash
# Listar modelos instalados
docker exec ollama-dev-ai-cpu ollama list

# Descargar un nuevo modelo
docker exec ollama-dev-ai-cpu ollama pull modelo:tag

# Eliminar un modelo
docker exec ollama-dev-ai-cpu ollama rm modelo:tag

# Chat interactivo con un modelo
docker exec -it ollama-dev-ai-cpu ollama run qwen2.5-coder:7b
```

## üí° Comandos Personalizados para Continue

La configuraci√≥n incluye comandos personalizados:

- `/explain`: Explica c√≥digo en espa√±ol
- `/optimize`: Optimiza c√≥digo para rendimiento
- `/fix`: Encuentra y corrige errores
- `/test`: Genera tests unitarios
- `/document`: Genera documentaci√≥n completa
- `/refactor`: Refactoriza siguiendo mejores pr√°cticas

## ‚ö° Recomendaciones de Uso

### Para Mejor Rendimiento:
- **C√≥digo simple**: Usa `qwen2.5-coder:7b`
- **C√≥digo complejo**: Usa `qwen2.5-coder:32b`
- **Explicaciones**: Usa `llama3.1:8b`
- **Debugging**: Usa `deepseek-coder:33b`

### Para Ahorrar Recursos:
- Usa solo los modelos que necesites
- Cierra VS Code cuando no lo uses para liberar memoria
- Considera usar modelos m√°s peque√±os en sistemas con poca RAM

## üîß Personalizaci√≥n

### Cambiar Configuraci√≥n de Continue
Edita `~/.continue/config.json` para:
- Ajustar temperatura (creatividad)
- Cambiar modelo por defecto
- Agregar comandos personalizados
- Modificar prompts

### Agregar Nuevos Modelos
```bash
# Buscar modelos disponibles
docker exec ollama-dev-ai-cpu ollama search coder

# Descargar modelo espec√≠fico
docker exec ollama-dev-ai-cpu ollama pull nuevo-modelo:tag
```

## üêõ Soluci√≥n de Problemas

### Puerto 11434 u otros puertos ocupados

Si recibes errores de puertos ocupados (11434, 3001):

```bash
# Opci√≥n 1: Usar el script de limpieza autom√°tica
./cleanup-ai.sh

# Opci√≥n 2: Liberar puertos manualmente
sudo netstat -tlnp | grep -E "(11434|3001)"
sudo pkill -f "docker-proxy.*(11434|3001)"

# Opci√≥n 3: Reiniciar Docker
sudo systemctl restart docker
```

### Configuraci√≥n de GPU NVIDIA en Fedora

Si tienes problemas con GPU en Fedora:

```bash
# Instalar NVIDIA Container Toolkit
curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

# Remover conflictos (si existen)
sudo dnf remove golang-github-nvidia-container-toolkit -y

# Instalar el toolkit
sudo dnf install -y nvidia-container-toolkit

# Configurar Docker
sudo nvidia-ctk runtime configure --runtime=docker

# Reiniciar Docker
sudo systemctl restart docker

# Probar GPU
docker run --rm --runtime=nvidia --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

### Error de Memoria
Si recibes errores de memoria:
1. Reduce el n√∫mero de modelos cargados
2. Usa modelos m√°s peque√±os (7B en lugar de 32B)
3. Aumenta la memoria virtual (swap)

### Modelos No Responden
1. Verifica que el contenedor est√© corriendo: `docker ps`
2. Revisa los logs: `docker-compose logs ollama`
3. Reinicia el servicio: `docker-compose restart`

### VS Code No Conecta
1. Verifica que Ollama est√© en http://localhost:11434
2. Comprueba la configuraci√≥n de Continue
3. Reinicia la extensi√≥n

## üìà Pr√≥ximos Pasos

1. **Explora la interfaz web** en http://localhost:3001
2. **Personaliza los prompts** seg√∫n tus necesidades
3. **Prueba diferentes modelos** para distintas tareas
4. **Configura atajos de teclado** personalizados en VS Code

## ü§ù Contribuir

¬øTienes sugerencias o mejoras? ¬°Abre un issue o env√≠a un PR!

---

**¬°Disfruta de tu IA local para desarrollo! üéâ**
