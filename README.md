# IA Local para Desarrollo con Docker ü§ñ

Esta configuraci√≥n permite ejecutar una IA local potente para desarrollo usando Docker y los mejores modelos de c√≥digo abierto disponibles.

## üöÄ Caracter√≠sticas

- **Ollama** como servidor de IA local (v0.9.6)
- **Modelos de c√≥digo abierto** m√°s avanzados para programaci√≥n
- **Interfaz web** OpenWebUI para gesti√≥n intuitiva de modelos
- **Configuraci√≥n autom√°tica** con detecci√≥n de GPU
- **Compatible con GPU NVIDIA y CPU**
- **Integraci√≥n perfecta con VS Code** v√≠a Continue
- **Scripts automatizados** para instalaci√≥n y mantenimiento

## ‚úÖ Estado del Proyecto (Verificado)

Este proyecto ha sido **probado y verificado** el 31 de julio de 2025 con:

- ‚úÖ **Docker 28.3.2** + **Docker Compose v2.38.2**
- ‚úÖ **Fedora 42 Workstation** (Kernel 6.15.7)
- ‚úÖ **NVIDIA GeForce RTX 4060 Laptop GPU** (8GB VRAM)
- ‚úÖ **Ollama v0.9.6** ejecut√°ndose correctamente
- ‚úÖ **OpenWebUI** funcionando en puerto 3001
- ‚úÖ **Modelos activos**: Qwen 2.5 Coder 7B + Llama 3.1 8B
- ‚úÖ **API de Ollama** respondiendo en puerto 11434
- ‚úÖ **Continue configurado** para VS Code

### Rendimiento Verificado

- **Tiempo de inicio**: ~3 segundos para cargar modelo en GPU
- **Memoria GPU utilizada**: ~450MB para cache KV del modelo 7B
- **Respuesta t√≠pica**: 4-7 segundos para consultas de c√≥digo


## üìã Requisitos Previos

### Requisitos M√≠nimos del Sistema

- **Docker**: versi√≥n 20.10+ (probado con 28.3.2)
- **Docker Compose**: v2.0+ (probado con v2.38.2)
- **RAM**: M√≠nimo 8GB (16GB recomendado para m√∫ltiples modelos)
- **Espacio en disco**: 50GB libres (para modelos y cache)
- **Sistema operativo**: Linux (probado en Fedora 42), macOS, Windows con WSL2

### Para GPU NVIDIA (Opcional pero Recomendado)

- **GPU**: NVIDIA con al menos 6GB VRAM (probado con RTX 4060 Laptop - 8GB)
- **Drivers**: NVIDIA drivers actualizados
- **NVIDIA Container Toolkit**: Para acelerar la inferencia

### Instalaci√≥n del NVIDIA Container Toolkit (Fedora)

```bash
# Agregar repositorio
curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

# Instalar toolkit
sudo dnf install -y nvidia-container-toolkit

# Configurar Docker
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# Verificar instalaci√≥n
docker run --rm --runtime=nvidia --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

## üõ†Ô∏è Instalaci√≥n R√°pida

### Opci√≥n 1: Configuraci√≥n Autom√°tica (Recomendada) ‚ö°

Para una configuraci√≥n autom√°tica con detecci√≥n inteligente de GPU:

```bash
./auto-setup.sh
```

**Caracter√≠sticas:**
- ‚úÖ Detecci√≥n autom√°tica de GPU NVIDIA
- ‚úÖ Limpieza autom√°tica de puertos ocupados
- ‚úÖ Descarga de modelos esenciales
- ‚úÖ Configuraci√≥n de Continue para VS Code
- ‚è±Ô∏è Tiempo: 15-30 minutos

### Opci√≥n 2: Instalaci√≥n Manual

Para usuarios que prefieren control total del proceso:

```bash
# 1. Levantar con GPU (si tienes NVIDIA)
docker compose --profile gpu up -d

# 2. O levantar con CPU solamente
docker compose --profile cpu up -d

# 3. Instalar modelos esenciales
docker exec ollama-dev-ai ollama pull qwen2.5-coder:7b
docker exec ollama-dev-ai ollama pull llama3.1:8b
```

### Opci√≥n 3: Solo Limpieza

Si necesitas limpiar la configuraci√≥n anterior:

```bash
./cleanup-ai.sh
```

## üß† Modelos Incluidos

### Modelos Verificados y Funcionando

Estos modelos han sido probados y est√°n funcionando correctamente en el sistema:

- **Qwen 2.5 Coder 7B**: Especializado en c√≥digo, r√°pido y eficiente (4.68 GB)
- **Llama 3.1 8B**: Modelo de uso general para explicaciones y consultas (4.92 GB)

### Modelos Adicionales Disponibles

Puedes instalar estos modelos seg√∫n tus necesidades:

- **Qwen 2.5 Coder 32B**: El m√°s avanzado para tareas complejas de c√≥digo (~20 GB)
- **CodeLlama 7B**: Especialista en m√∫ltiples lenguajes de programaci√≥n
- **DeepSeek Coder 6.7B**: Excelente para an√°lisis y generaci√≥n de c√≥digo
- **Llama 3.2 1B**: Modelo ligero para respuestas r√°pidas

## üîß Configuraci√≥n en VS Code

### Opci√≥n 1: Continue con Agentes (Recomendada) ü§ñ

1. Instala la extensi√≥n **Continue** desde el marketplace
2. Ejecuta el script de configuraci√≥n autom√°tica:
```bash
./setup-continue-agents.sh
```
3. Reinicia VS Code
4. Abre Continue con `Ctrl+Shift+P` > "Continue: Open"
5. **¬°Ahora ver√°s la secci√≥n de Agentes!** Incluye:
   - **Asistente de C√≥digo**: Especialista en desarrollo y debugging
   - **Revisor de C√≥digo**: Analiza errores y vulnerabilidades
   - **Documentador**: Genera documentaci√≥n t√©cnica

### Verificar Configuraci√≥n de Agentes
```bash
./check-continue-status.sh
```

### Opci√≥n 2: Configuraci√≥n Manual

1. Copia el archivo `continue-config.json` a `~/.continue/config.json`
2. Reinicia VS Code
3. Usa `Ctrl+I` para chat inline o `Ctrl+Shift+P` > "Continue: Open"

### Opci√≥n 2: Otras extensiones compatibles

- **Codeium**: Configura con URL `http://localhost:11434`
- **Tabnine**: Usar modo local con la API de Ollama
- **GitHub Copilot Chat**: Configurar con endpoint local

## üåê URLs de Acceso

- **API de Ollama**: http://localhost:11434
- **Interfaz Web**: http://localhost:3001
- **Documentaci√≥n API**: http://localhost:11434/api/tags

## üìù Comandos √ötiles

### Gesti√≥n de Contenedores
```bash
# Iniciar servicios
docker compose --profile cpu up -d  # Para CPU
docker compose --profile gpu up -d  # Para GPU

# Parar servicios
docker compose down

# Ver logs
docker compose logs -f

# Reiniciar servicios
docker compose restart
```

### Gesti√≥n de Modelos
```bash
# Listar modelos instalados
docker exec ollama-dev-ai-cpu ollama list

# Descargar un nuevo modelo
docker exec ollama-dev-ai-cpu ollama pull modelo:tag

# Eliminar un modelo
docker exec ollama-dev-ai-cpu ollama rm modelo:tag

# Chat interactivo con un modelo
docker exec -it ollama-dev-ai-cpu ollama run qwen2.5-coder:7b
```

## üí° Comandos Personalizados para Continue

La configuraci√≥n incluye comandos personalizados:

- `/explain`: Explica c√≥digo en espa√±ol
- `/optimize`: Optimiza c√≥digo para rendimiento
- `/fix`: Encuentra y corrige errores
- `/test`: Genera tests unitarios
- `/document`: Genera documentaci√≥n completa
- `/refactor`: Refactoriza siguiendo mejores pr√°cticas

## ‚ö° Recomendaciones de Uso

### Para Mejor Rendimiento:
- **C√≥digo simple**: Usa `qwen2.5-coder:7b`
- **C√≥digo complejo**: Usa `qwen2.5-coder:32b`
- **Explicaciones**: Usa `llama3.1:8b`
- **Debugging**: Usa `deepseek-coder:33b`

### Para Ahorrar Recursos:
- Usa solo los modelos que necesites
- Cierra VS Code cuando no lo uses para liberar memoria
- Considera usar modelos m√°s peque√±os en sistemas con poca RAM

## üîß Personalizaci√≥n

### Cambiar Configuraci√≥n de Continue
Edita `~/.continue/config.json` para:
- Ajustar temperatura (creatividad)
- Cambiar modelo por defecto
- Agregar comandos personalizados
- Modificar prompts

### Agregar Nuevos Modelos
```bash
# Buscar modelos disponibles
docker exec ollama-dev-ai-cpu ollama search coder

# Descargar modelo espec√≠fico
docker exec ollama-dev-ai-cpu ollama pull nuevo-modelo:tag
```

## üêõ Soluci√≥n de Problemas

### Verificaci√≥n R√°pida del Sistema

Usa el script de verificaci√≥n incluido:

```bash
./check-continue-status.sh
```

Este script verifica autom√°ticamente:
- Estado de contenedores Docker
- Conectividad con Ollama API
- Configuraci√≥n de Continue
- Modelos disponibles

### Puerto 11434 u otros puertos ocupados

Si recibes errores de puertos ocupados:

```bash
# Opci√≥n 1: Usar script de limpieza (recomendado)
./cleanup-ai.sh

# Opci√≥n 2: Verificar manualmente qu√© usa los puertos
sudo netstat -tlnp | grep -E "(11434|3001)"

# Opci√≥n 3: Forzar liberaci√≥n de puertos
sudo pkill -f "docker-proxy.*(11434|3001)"
```

### Problemas con GPU NVIDIA

Si el sistema no detecta tu GPU:

```bash
# Verificar que la GPU es visible
nvidia-smi

# Verificar Docker + NVIDIA
docker run --rm --runtime=nvidia --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi

# Si falla, reinstalar NVIDIA Container Toolkit
sudo dnf install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

### Error "No such profile: gpu"

Si recibes este error, usa el perfil CPU:

```bash
docker compose --profile cpu up -d
```

### Modelos No Cargan o Son Lentos

1. **Verificar memoria disponible**:
   ```bash
   free -h
   # GPU memory
   nvidia-smi
   ```

2. **Usar modelos m√°s peque√±os**:
   ```bash
   # En lugar de modelos 32B, usa 7B
   docker exec ollama-dev-ai ollama pull qwen2.5-coder:7b
   ```

3. **Verificar logs de Ollama**:
   ```bash
   docker compose logs --tail=20 ollama
   ```

## üìà Pr√≥ximos Pasos

### Despu√©s de la Instalaci√≥n

1. **Accede a la interfaz web**: <http://localhost:3001>
2. **Instala Continue en VS Code** desde el marketplace
3. **Copia la configuraci√≥n**: `cp continue-config.json ~/.continue/config.json`
4. **Reinicia VS Code** y prueba `Ctrl+I` para chat inline

### Personalizaci√≥n Avanzada

1. **Instala modelos adicionales** seg√∫n tus necesidades:
   ```bash
   # Para proyectos m√°s complejos
   docker exec ollama-dev-ai ollama pull qwen2.5-coder:32b
   
   # Para desarrollo web
   docker exec ollama-dev-ai ollama pull codellama:7b
   ```

2. **Configura comandos personalizados** editando `continue-config.json`
3. **Ajusta la temperatura** del modelo para creatividad vs precisi√≥n
4. **Configura atajos de teclado** en VS Code para acceso r√°pido

### Monitoreo y Optimizaci√≥n

- **Monitorea el uso de memoria**: `docker stats`
- **Revisa logs peri√≥dicamente**: `docker compose logs --tail=50`
- **Actualiza modelos**: Los nuevos modelos se lanzan frecuentemente

## ü§ù Contribuir

¬øTienes sugerencias o mejoras? ¬°Abre un issue o env√≠a un PR!

---

### ¬°Disfruta de tu IA local para desarrollo! üéâ

> Proyecto verificado y funcionando el 31 de julio de 2025
> Sistema probado: Fedora 42 + Docker 28.3.2 + RTX 4060 Laptop GPU
