# IA Local para Desarrollo con Docker ü§ñ

Esta configuraci√≥n te permite ejecutar una IA local potente para desarrollo usando Docker y los mejores modelos de c√≥digo abierto disponibles.

## üöÄ Caracter√≠sticas

- **Ollama** como servidor de IA local
- **Modelos de c√≥digo abierto** m√°s avanzados para programaci√≥n
- **Interfaz web** para gesti√≥n de modelos
- **Configuraci√≥n autom√°tica** con un solo comando
- **Compatible con GPU y CPU**
- **Integraci√≥n perfecta con VS Code**

## üìã Requisitos Previos

- Docker y Docker Compose instalados
- Al menos 8GB de RAM (16GB recomendado)
- 50GB de espacio libre en disco
- (Opcional) GPU NVIDIA con drivers instalados para mejor rendimiento

## üõ†Ô∏è Instalaci√≥n R√°pida

1. Ejecuta el script de configuraci√≥n:
```bash
./setup-ai.sh
```

2. Espera a que se descarguen todos los modelos (puede tomar 30-60 minutos)

3. ¬°Ya tienes tu IA local funcionando!

## üß† Modelos Incluidos

### Modelos Especializados en C√≥digo:
- **Qwen 2.5 Coder 32B**: El m√°s avanzado para tareas complejas de c√≥digo
- **Qwen 2.5 Coder 7B**: R√°pido y eficiente para c√≥digo simple
- **CodeLlama 34B**: Especialista en m√∫ltiples lenguajes de programaci√≥n
- **DeepSeek Coder 33B**: Excelente para an√°lisis y generaci√≥n de c√≥digo
- **Granite Code 34B**: Modelo de IBM optimizado para desarrollo

### Modelo de Uso General:
- **Llama 3.1 8B**: R√°pido para consultas generales y explicaciones

## üîß Configuraci√≥n en VS Code

### Opci√≥n 1: Continue (Recomendada)

1. Instala la extensi√≥n **Continue** desde el marketplace
2. Copia el archivo `continue-config.json` a `~/.continue/config.json`
3. Reinicia VS Code
4. Usa `Ctrl+I` para chat inline o `Ctrl+Shift+P` > "Continue: Open"

### Opci√≥n 2: Otras extensiones compatibles

- **Codeium**: Configura con URL `http://localhost:11434`
- **Tabnine**: Usar modo local con la API de Ollama
- **GitHub Copilot Chat**: Configurar con endpoint local

## üåê URLs de Acceso

- **API de Ollama**: http://localhost:11434
- **Interfaz Web**: http://localhost:3000
- **Documentaci√≥n API**: http://localhost:11434/api/tags

## üìù Comandos √ötiles

### Gesti√≥n de Contenedores
```bash
# Iniciar servicios
docker compose --profile cpu up -d  # Para CPU
docker compose --profile gpu up -d  # Para GPU

# Parar servicios
docker compose down

# Ver logs
docker compose logs -f

# Reiniciar servicios
docker compose restart
```

### Gesti√≥n de Modelos
```bash
# Listar modelos instalados
docker exec ollama-dev-ai-cpu ollama list

# Descargar un nuevo modelo
docker exec ollama-dev-ai-cpu ollama pull modelo:tag

# Eliminar un modelo
docker exec ollama-dev-ai-cpu ollama rm modelo:tag

# Chat interactivo con un modelo
docker exec -it ollama-dev-ai-cpu ollama run qwen2.5-coder:7b
```

## üí° Comandos Personalizados para Continue

La configuraci√≥n incluye comandos personalizados:

- `/explain`: Explica c√≥digo en espa√±ol
- `/optimize`: Optimiza c√≥digo para rendimiento
- `/fix`: Encuentra y corrige errores
- `/test`: Genera tests unitarios
- `/document`: Genera documentaci√≥n completa
- `/refactor`: Refactoriza siguiendo mejores pr√°cticas

## ‚ö° Recomendaciones de Uso

### Para Mejor Rendimiento:
- **C√≥digo simple**: Usa `qwen2.5-coder:7b`
- **C√≥digo complejo**: Usa `qwen2.5-coder:32b`
- **Explicaciones**: Usa `llama3.1:8b`
- **Debugging**: Usa `deepseek-coder:33b`

### Para Ahorrar Recursos:
- Usa solo los modelos que necesites
- Cierra VS Code cuando no lo uses para liberar memoria
- Considera usar modelos m√°s peque√±os en sistemas con poca RAM

## üîß Personalizaci√≥n

### Cambiar Configuraci√≥n de Continue
Edita `~/.continue/config.json` para:
- Ajustar temperatura (creatividad)
- Cambiar modelo por defecto
- Agregar comandos personalizados
- Modificar prompts

### Agregar Nuevos Modelos
```bash
# Buscar modelos disponibles
docker exec ollama-dev-ai-cpu ollama search coder

# Descargar modelo espec√≠fico
docker exec ollama-dev-ai-cpu ollama pull nuevo-modelo:tag
```

## üêõ Soluci√≥n de Problemas

### Error de Memoria
Si recibes errores de memoria:
1. Reduce el n√∫mero de modelos cargados
2. Usa modelos m√°s peque√±os (7B en lugar de 32B)
3. Aumenta la memoria virtual (swap)

### Modelos No Responden
1. Verifica que el contenedor est√© corriendo: `docker ps`
2. Revisa los logs: `docker-compose logs ollama`
3. Reinicia el servicio: `docker-compose restart`

### VS Code No Conecta
1. Verifica que Ollama est√© en http://localhost:11434
2. Comprueba la configuraci√≥n de Continue
3. Reinicia la extensi√≥n

## üìà Pr√≥ximos Pasos

1. **Explora la interfaz web** en http://localhost:3000
2. **Personaliza los prompts** seg√∫n tus necesidades
3. **Prueba diferentes modelos** para distintas tareas
4. **Configura atajos de teclado** personalizados en VS Code

## ü§ù Contribuir

¬øTienes sugerencias o mejoras? ¬°Abre un issue o env√≠a un PR!

---

**¬°Disfruta de tu IA local para desarrollo! üéâ**
